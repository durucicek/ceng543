{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a81a22",
   "metadata": {},
   "source": [
    "# Question 4 - Retrieval-Augmented Generation (RAG) and Knowledge-Grounded Text Synthesis\n",
    "\n",
    "Here, in order to construct a RAG system, we have chosen:\n",
    "* The retriever BM25 (sparse)\n",
    "* The retriever all-MiniLM-L6-v2 (sentcence-bert, dense)\n",
    "* The generator FLAN-T5 (small)\n",
    "* Applied to HotpotQA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29e02f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from bert_score import score as bert_score_func\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Download NLTK data for tokenizer\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6c160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a24ded6d",
   "metadata": {},
   "source": [
    "### a-b ) Construct RAG system with BM25, S-BERT, FLAN-T5, applied to HotpotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "642f2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "MODEL_NAME = \"google/flan-t5-small\"  # Efficient instruction-tuned model\n",
    "SUBSET_SIZE = 5000  \n",
    "TOP_K = 3          # Number of documents to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ad03d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HotpotQA dataset (distractor subset)...\n",
      "Processing corpus...\n",
      "Processed 5000 queries against 49774 documents.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "print(\"Loading HotpotQA dataset (distractor subset)...\")\n",
    "# We use the 'distractor' set which contains the question and supporting paragraphs\n",
    "dataset = load_dataset(\"hotpot_qa\", \"distractor\", split=f\"validation[:{SUBSET_SIZE}]\")\n",
    "\n",
    "# We need to flatten the dataset to create a \"Knowledge Base\" (Corpus)\n",
    "# HotpotQA structure: 'context' is a list of [title, sentences]\n",
    "corpus = []\n",
    "doc_ids = []\n",
    "queries = []\n",
    "ground_truth_answers = []\n",
    "ground_truth_titles = []  # To check retrieval accuracy\n",
    "\n",
    "print(\"Processing corpus...\")\n",
    "for item in dataset:\n",
    "    # 1. Extract Queries and Answers\n",
    "    queries.append(item[\"question\"])\n",
    "    ground_truth_answers.append(item[\"answer\"])\n",
    "    \n",
    "    # 2. Extract Ground Truth Titles (for Retrieval Evaluation)\n",
    "    # In the HF version, supporting_facts is often a dict:\n",
    "    #   {'title': [...], 'sent_id': [...]}\n",
    "    sf = item[\"supporting_facts\"]\n",
    "    \n",
    "    if isinstance(sf, dict) and \"title\" in sf:\n",
    "        # Newer HF format\n",
    "        titles = sf[\"title\"]\n",
    "    else:\n",
    "        # Fallback: older format as list of [title, sent_id]\n",
    "        titles = [fact[0] for fact in sf]\n",
    "    \n",
    "    gt_titles_set = set(titles)\n",
    "    ground_truth_titles.append(gt_titles_set)\n",
    "    \n",
    "    # 3. Build Corpus from 'context'\n",
    "    # Each item has multiple paragraphs. We treat each paragraph as a document.\n",
    "    for title, sentences in zip(item[\"context\"][\"title\"], item[\"context\"][\"sentences\"]):\n",
    "        text = \" \".join(sentences)\n",
    "        corpus.append(text)\n",
    "        doc_ids.append(title)  # Use title as the doc ID\n",
    "\n",
    "print(f\"Processed {len(queries)} queries against {len(corpus)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b4382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVERS\n",
    "\n",
    "class SparseRetriever:\n",
    "    def __init__(self, corpus):\n",
    "        print(\"Initializing BM25 (Sparse) Retriever...\")\n",
    "        # Tokenize corpus for BM25\n",
    "        tokenized_corpus = [nltk.word_tokenize(doc.lower()) for doc in corpus]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def retrieve(self, query, top_k=3):\n",
    "        tokenized_query = nltk.word_tokenize(query.lower())\n",
    "        # Get top-k indices\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_n_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        return [self.corpus[i] for i in top_n_indices], [doc_ids[i] for i in top_n_indices]\n",
    "\n",
    "class DenseRetriever:\n",
    "    def __init__(self, corpus):\n",
    "        print(\"Initializing SBERT (Dense) Retriever...\")\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
    "        self.corpus = corpus\n",
    "        # Pre-compute corpus embeddings\n",
    "        self.corpus_embeddings = self.model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
    "        \n",
    "    def retrieve(self, query, top_k=3):\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        # Cosine Similarity\n",
    "        hits = util.semantic_search(query_embedding, self.corpus_embeddings, top_k=top_k)[0]\n",
    "        # hits is list of {corpus_id, score}\n",
    "        indices = [hit['corpus_id'] for hit in hits]\n",
    "        return [self.corpus[i] for i in indices], [doc_ids[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cdfd13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BM25 (Sparse) Retriever...\n",
      "Initializing SBERT (Dense) Retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1556/1556 [00:29<00:00, 52.03it/s] \n"
     ]
    }
   ],
   "source": [
    "# Initialize Retrievers\n",
    "sparse_retriever = SparseRetriever(corpus)\n",
    "dense_retriever = DenseRetriever(corpus) # Optional Task b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b83af1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Module\n",
    "\n",
    "class RAGGenerator:\n",
    "    def __init__(self):\n",
    "        print(f\"Loading Generator ({MODEL_NAME})...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "    def generate(self, query, context_docs):\n",
    "        # Concatenate retrieved docs into a context block\n",
    "        context_text = \" \".join(context_docs)\n",
    "        \n",
    "        # FLAN-T5 Prompt Template\n",
    "        prompt = f\"Use the context below to answer the question.\\n\\nContext: {context_text}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=50, \n",
    "            num_beams=2, \n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8b8e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Generator (google/flan-t5-small)...\n"
     ]
    }
   ],
   "source": [
    "generator = RAGGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719ff6d",
   "metadata": {},
   "source": [
    "### c) Evaluate retrieval and generation components separately and jointly using Precision@k, Recall@k, BLEU, ROUGE-L, and BERTScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c7ed51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(retrieval_results, generation_results, gt_titles_list, gt_answers_list):\n",
    "    # 1. Retrieval Metrics\n",
    "    recall_at_k = 0\n",
    "    precision_scores = []\n",
    "    \n",
    "    for retrieved_ids, true_ids in zip(retrieval_results['ids'], gt_titles_list):\n",
    "        retrieved_set = set(retrieved_ids)\n",
    "        true_set = set(true_ids)\n",
    "        \n",
    "        # Intersection between retrieved titles and ground-truth titles\n",
    "        intersection = retrieved_set.intersection(true_set)\n",
    "        \n",
    "        # Recall@K (relaxed: did we retrieve at least one relevant doc?)\n",
    "        if len(intersection) > 0:\n",
    "            recall_at_k += 1\n",
    "        \n",
    "        # Precision@K = (# relevant retrieved) / (# retrieved)\n",
    "        if len(retrieved_ids) > 0:\n",
    "            precision_scores.append(len(intersection) / len(retrieved_ids))\n",
    "        else:\n",
    "            precision_scores.append(0.0)\n",
    "            \n",
    "    avg_recall = recall_at_k / len(gt_titles_list)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    \n",
    "    # 2. Generation Metrics (BLEU, ROUGE)\n",
    "    rouge = Rouge()\n",
    "    bleu_scores = []\n",
    "    rouge_l_scores = []\n",
    "    \n",
    "    preds_for_bert = generation_results\n",
    "    refs_for_bert = gt_answers_list\n",
    "    \n",
    "    for pred, ref in zip(generation_results, gt_answers_list):\n",
    "        if len(pred.strip()) == 0:\n",
    "            pred = \"empty\"  # Prevent crash\n",
    "\n",
    "        bleu = sentence_bleu(\n",
    "            [nltk.word_tokenize(ref)],\n",
    "            nltk.word_tokenize(pred)\n",
    "        )\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "        try:\n",
    "            r_scores = rouge.get_scores(pred, ref)[0]\n",
    "            rouge_l_scores.append(r_scores['rouge-l']['f'])\n",
    "        except:\n",
    "            rouge_l_scores.append(0.0)\n",
    "\n",
    "    try:\n",
    "        P, R, F1 = bert_score_func(preds_for_bert, refs_for_bert,\n",
    "                                   lang=\"en\", verbose=False,\n",
    "                                   device=DEVICE.type)\n",
    "        bert_score_avg = F1.mean().item()\n",
    "    except Exception as e:\n",
    "        print(f\"BERTScore failed (likely memory): {e}\")\n",
    "        bert_score_avg = 0.0\n",
    "\n",
    "    return {\n",
    "        \"Recall@K\": avg_recall,\n",
    "        \"Precision@K\": avg_precision,\n",
    "        \"BLEU\": np.mean(bleu_scores),\n",
    "        \"ROUGE-L\": np.mean(rouge_l_scores),\n",
    "        \"BERTScore\": bert_score_avg\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f00d5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running RAG Pipeline ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [41:49<00:00,  1.99it/s]\n",
      "/home/duru.cicek/.local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/duru.cicek/.local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/duru.cicek/.local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG Performance Comparison ===\n",
      "               Recall@K  Precision@K      BLEU   ROUGE-L  BERTScore\n",
      "BM25 (Sparse)    0.8046     0.337200  0.026541  0.315914   0.884432\n",
      "SBERT (Dense)    0.8692     0.378733  0.025308  0.292598   0.880790\n"
     ]
    }
   ],
   "source": [
    "# EXECUTION LOOP\n",
    "results_log = {'sparse': {'docs': [], 'ids': [], 'ans': []}, \n",
    "               'dense': {'docs': [], 'ids': [], 'ans': []}}\n",
    "\n",
    "print(\"\\n--- Running RAG Pipeline ---\")\n",
    "for i in tqdm(range(len(queries))):\n",
    "    q = queries[i]\n",
    "    \n",
    "    # 1. Sparse Path\n",
    "    s_docs, s_ids = sparse_retriever.retrieve(q, TOP_K)\n",
    "    s_ans = generator.generate(q, s_docs)\n",
    "    results_log['sparse']['docs'].append(s_docs)\n",
    "    results_log['sparse']['ids'].append(s_ids)\n",
    "    results_log['sparse']['ans'].append(s_ans)\n",
    "    \n",
    "    # 2. Dense Path\n",
    "    d_docs, d_ids = dense_retriever.retrieve(q, TOP_K)\n",
    "    d_ans = generator.generate(q, d_docs)\n",
    "    results_log['dense']['docs'].append(d_docs)\n",
    "    results_log['dense']['ids'].append(d_ids)\n",
    "    results_log['dense']['ans'].append(d_ans)\n",
    "\n",
    "# Calculate Metrics\n",
    "metrics_sparse = calculate_metrics(results_log['sparse'], results_log['sparse']['ans'], ground_truth_titles, ground_truth_answers)\n",
    "metrics_dense = calculate_metrics(results_log['dense'], results_log['dense']['ans'], ground_truth_titles, ground_truth_answers)\n",
    "\n",
    "# Display Results\n",
    "results_df = pd.DataFrame([metrics_sparse, metrics_dense], index=['BM25 (Sparse)', 'SBERT (Dense)'])\n",
    "print(\"\\n=== RAG Performance Comparison ===\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed36e4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Qualitative Analysis: Faithful vs. Hallucinated ===\n",
      "\n",
      "--- FAITHFUL / SUCCESSFUL GENERATION ---\n",
      "Question: Are the Laleli Mosque and Esma Sultan Mansion located in the same neighborhood?\n",
      "Ground Truth: no\n",
      "Retrieved Context (Top 1): The Esma Sultan Mansion (Turkish: \"Esma Sultan Yalısı\" ), a historical yalı (English: waterside mansion ) located at Bosphorus in Ortaköy neighborhood of Istanbul, Turkey and named after its original ...\n",
      "RAG Prediction: no\n",
      "\n",
      "--- HALLUCINATED / FAILED GENERATION ---\n",
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "Ground Truth: yes\n",
      "Retrieved Context (Top 1): Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.  The film concerns the period in Wood's life w...\n",
      "RAG Prediction: no\n"
     ]
    }
   ],
   "source": [
    "#QUALITATIVE EXAMPLES (Task d)\n",
    "print(\"\\n=== Qualitative Analysis: Faithful vs. Hallucinated ===\")\n",
    "\n",
    "# Find a good example (high ROUGE) and a bad example (low ROUGE) from Dense results\n",
    "rouge = Rouge()\n",
    "best_score = -1\n",
    "worst_score = 100\n",
    "best_idx = 0\n",
    "worst_idx = 0\n",
    "\n",
    "for i, (pred, ref) in enumerate(zip(results_log['dense']['ans'], ground_truth_answers)):\n",
    "    try:\n",
    "        score = rouge.get_scores(pred, ref)[0]['rouge-l']['f']\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_idx = i\n",
    "        if score < worst_score:\n",
    "            worst_score = score\n",
    "            worst_idx = i\n",
    "    except: pass\n",
    "\n",
    "def print_example(idx, label):\n",
    "    print(f\"\\n--- {label} GENERATION ---\")\n",
    "    print(f\"Question: {queries[idx]}\")\n",
    "    print(f\"Ground Truth: {ground_truth_answers[idx]}\")\n",
    "    print(f\"Retrieved Context (Top 1): {results_log['dense']['docs'][idx][0][:200]}...\")\n",
    "    print(f\"RAG Prediction: {results_log['dense']['ans'][idx]}\")\n",
    "\n",
    "print_example(best_idx, \"FAITHFUL / SUCCESSFUL\")\n",
    "print_example(worst_idx, \"HALLUCINATED / FAILED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "543",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
